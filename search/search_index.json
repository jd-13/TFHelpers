{
    "docs": [
        {
            "location": "/",
            "text": "TFHelpers\n\n\nTFHelpers is a small collection of classes which implement common tasks in Tensorflow. The\nfunctionality provided by each module is described in this documentation.\n\n\nThe modules \nFilesAndLogging\n and \nTrainingHelpers\n provide functionality such as managing model\nfiles and early stopping, and may be useful in many tensorflow models. The module \nScikitWrapper\n\nprovides several classes which aid with implementing tensorflow models behind a scikit-learn style\nAPI.\n\n\nIf you use the \nFilesAndLogging\n module you'll also be able to use the \nModelManager\n web interface\nto sort your models by hyperparameters and generate the appropriate tensorboard commands to compare\ndifferent models and training runs.",
            "title": "Home"
        },
        {
            "location": "/#tfhelpers",
            "text": "TFHelpers is a small collection of classes which implement common tasks in Tensorflow. The\nfunctionality provided by each module is described in this documentation.  The modules  FilesAndLogging  and  TrainingHelpers  provide functionality such as managing model\nfiles and early stopping, and may be useful in many tensorflow models. The module  ScikitWrapper \nprovides several classes which aid with implementing tensorflow models behind a scikit-learn style\nAPI.  If you use the  FilesAndLogging  module you'll also be able to use the  ModelManager  web interface\nto sort your models by hyperparameters and generate the appropriate tensorboard commands to compare\ndifferent models and training runs.",
            "title": "TFHelpers"
        },
        {
            "location": "/SKTFModels/",
            "text": "SKTFModels\n\n\nThis module provides examples of models built using the \nSciKitWrapper\n module. This is a good place\nto start if you wish to implement your own models using this wrapper.\n\n\nBasicRegressor\n\n\nCurrently this module provides only the \nBasicRegressor\n class, which inherits from \nTFRegressor\n\nand implements a basic multilayer feedforward network for regressions. Its hyperparameters are to be\npassed to the constructor as follows, and provide the following options:\n\n\nBasicRegressor(self,\n               learningRate=0.01,\n               batchSize=1000,\n               initializer=tf.contrib.layers.variance_scaling_initializer(),\n               dropoutRate=0.01,\n               restoreFrom=None,\n               hiddenNeuronsList=[10])\n\n\n\n\n\nlearningRate\n: Provided to the gradient descent algorithm, in this case \ntf.train.AdamOptimizer\n\n\nbatchSize\n: Number of rows of data to operate on at once\n\n\ninitializer\n: The initializer to use as the kernel initializer for each layer\n\n\ndropoutRate\n: Not yet implemented in this model, does nothing\n\n\nrestoreFrom\n: If this model has been trained previously with the same hyperparameters, provide\nthe date time string of the form YYYYMMDD-HHmm which corresponds to the previous training run. The\n\nCheckpointAndRestoreHelper\n will then look this up from the models directory.\n\n\nhiddenNeuronsList\n: A list of integers which describes the number of neurons in each hidden layer.\nEg. [100, 50, 30] would mean 100 neurons in the first layer, 50 in the second and 30 in the third\nlayer.",
            "title": "SKTFModels"
        },
        {
            "location": "/SKTFModels/#sktfmodels",
            "text": "This module provides examples of models built using the  SciKitWrapper  module. This is a good place\nto start if you wish to implement your own models using this wrapper.",
            "title": "SKTFModels"
        },
        {
            "location": "/SKTFModels/#basicregressor",
            "text": "Currently this module provides only the  BasicRegressor  class, which inherits from  TFRegressor \nand implements a basic multilayer feedforward network for regressions. Its hyperparameters are to be\npassed to the constructor as follows, and provide the following options:  BasicRegressor(self,\n               learningRate=0.01,\n               batchSize=1000,\n               initializer=tf.contrib.layers.variance_scaling_initializer(),\n               dropoutRate=0.01,\n               restoreFrom=None,\n               hiddenNeuronsList=[10])   learningRate : Provided to the gradient descent algorithm, in this case  tf.train.AdamOptimizer  batchSize : Number of rows of data to operate on at once  initializer : The initializer to use as the kernel initializer for each layer  dropoutRate : Not yet implemented in this model, does nothing  restoreFrom : If this model has been trained previously with the same hyperparameters, provide\nthe date time string of the form YYYYMMDD-HHmm which corresponds to the previous training run. The CheckpointAndRestoreHelper  will then look this up from the models directory.  hiddenNeuronsList : A list of integers which describes the number of neurons in each hidden layer.\nEg. [100, 50, 30] would mean 100 neurons in the first layer, 50 in the second and 30 in the third\nlayer.",
            "title": "BasicRegressor"
        },
        {
            "location": "/ScikitLearnWrapper/",
            "text": "Scikit-learn Wrapper\n\n\nTFHelpers provides a basic wrapper class which you can inherit from to create models which are\ncompatible with the scikit-learn api. Models which inherit from the \nSKTFWrapper\n class will be\ncompatible with scikit-learn functionality such as \nGridSearchCV\n, and will provide \nfit\n and\n\npredict\n methods for training and producing predictions.\n\n\nIf you inherit from \nSKTFWrapper\n you will need to build the graph and implement the training loop\nin the \nfit\n method. The \nSciKitWrapper\n module also contains child classes of \nSKTFWrapper\n which\nprovide further functionality. Currently only \nTFRegressor\n is available.\n\n\nTFRegressor\n\n\nIf you wish to create a regression model you can inherit from the \nTFRegressor\n class which provides\na lot of functionality, leaving the methods \n_buildGraph\n, \n_restoreGraph\n, \n_buildModelName\n, and\nthe constructor for you to implement.\n\n\nUsing \nTFRegressor\n gets you the following features for free:\n\n\n\n\nA fairly conventional training loop, with user specified batch size\n\n\nLogging of training and validation losses to tensorboard on each epoch, as well as writing any \ntf.Summary\n nodes at each epoch\n\n\nSaving the model at each epoch\n\n\nContinuing training from previous runs\n\n\nSaving all model and tensorboard related files in a directory structure sorted by model hyperparameters and start time\n\n\nEarly stopping\n\n\nPredicted time to completion\n\n\nWarnings if issues are detected during training such as: parts of the graph do not appear to be training, the loss is zero, or there is a risk of exploding gradients\n\n\n\n\nImplementation\n\n\n__init__(self,\n        learningRate,\n        batchSize,\n        initializer,\n        dropoutRate,\n        restoreFrom,\n        outputLength)\n\n\n\nThe \n__init__\n method should be overriden and used to set the hyperparameters for your own model,\nand should call \nTFRegressor.__init__\n to provide the hyperparameters required by the \nTFRegressor\n.\n\n\nNOTE: For compatitbility with scikit-learn functionality such as \nGridSearchCV\n, every parameter\npassed to the constructor of your model must be saved to a member variable of exactly the same name.\n\n\n_buildModelName(self)\n\n\n\nIn this method you will need to return a string which adequately describes your model and its\nhyperparameters. This string is used to group model files.\n\n\n_buildGraph(self, numFeatures)\n\n\n\nIn this method you will need to build the graph for your model, and return a \nRegressorTensors\n\nobject which contains the appropriate tensors/operations from your graph. This object is the\ninterface between the graph which you have created in \n_buildGraph\n and the training loop\nimplemented in \nTFRegressor\n. \n\n\nThe parameter \nnumFeatures\n is the number of columns in the parameter \nX\n provided to the \nfit\n\nmethod.\n\n\n_restoreGraph(self, graph)\n\n\n\nThis method must also return a \nRegressorTensors\n object, however the tensors provided to the\nconstructor of \nRegressorTensors\n must be recovered from the provided \ngraph\n using either\n\ngraph.get_tensor_by_name(...)\n or \ngraph.get_operation_by_name(...)\n.\n\n\nRegressorTensors(self,\n                 X_in,\n                 y_in,\n                 logits,\n                 loss,\n                 trainingOp,\n                 dropoutKeepProb)\n\n\n\nThe constructor of the \nRegressorTensors\n object takes several operations from your graph that the\ntraining loop will use while training the model. Each is explained below:\n\n\n\n\nX_in\n: The \ntf.Placeholder\n for your features\n\n\ny_in\n: The \ntf.Placeholder\n for your labels\n\n\nlogits\n: The output of your graph. The predict method will call \nlogits.eval(...)\n to produce predictions\n\n\nloss\n: The loss of your graph. \nloss.eval(...)\n will be called to create training and validation loss values to log in tensorboard\n\n\ntrainingOp\n: The operator that should be evaluated for each batch and epoch to train your model\n\n\ndropoutKeepProb\n: A \ntf.Placeholder\n which will be set to the dropout rate during training\n\n\n\n\nFor complete examples of all of the above methods, see \nSKTFModels.BasicRegressor\n.\n\n\nUsage\n\n\nThe interface to train models which inherit from \nTFRegressor\n is the \nfit\n and \npredict\n methods.\n\n\nfit(self, X, y, X_valid, y_valid, numEpochs)\n\n\n\nThis will call either \n_buildGraph\n or \n_restoreGraph\n and then train the model for \nnumEpochs\n,\nusing \nX\n as the features and \ny\n as the labels. A validation set must also be provided in \nX_valid\n\nand \ny_valid\n.\n\n\npredict(self, X)\n\n\n\nWill perform inference on the given dataset \nX\n, returning a numpy array of predictions. Handles an\n\nX\n that has a large number of parameters by splitting it into batches the same size as specified in\nthe constructor.",
            "title": "Scikit-learn Wrapper"
        },
        {
            "location": "/ScikitLearnWrapper/#scikit-learn-wrapper",
            "text": "TFHelpers provides a basic wrapper class which you can inherit from to create models which are\ncompatible with the scikit-learn api. Models which inherit from the  SKTFWrapper  class will be\ncompatible with scikit-learn functionality such as  GridSearchCV , and will provide  fit  and predict  methods for training and producing predictions.  If you inherit from  SKTFWrapper  you will need to build the graph and implement the training loop\nin the  fit  method. The  SciKitWrapper  module also contains child classes of  SKTFWrapper  which\nprovide further functionality. Currently only  TFRegressor  is available.",
            "title": "Scikit-learn Wrapper"
        },
        {
            "location": "/ScikitLearnWrapper/#tfregressor",
            "text": "If you wish to create a regression model you can inherit from the  TFRegressor  class which provides\na lot of functionality, leaving the methods  _buildGraph ,  _restoreGraph ,  _buildModelName , and\nthe constructor for you to implement.  Using  TFRegressor  gets you the following features for free:   A fairly conventional training loop, with user specified batch size  Logging of training and validation losses to tensorboard on each epoch, as well as writing any  tf.Summary  nodes at each epoch  Saving the model at each epoch  Continuing training from previous runs  Saving all model and tensorboard related files in a directory structure sorted by model hyperparameters and start time  Early stopping  Predicted time to completion  Warnings if issues are detected during training such as: parts of the graph do not appear to be training, the loss is zero, or there is a risk of exploding gradients",
            "title": "TFRegressor"
        },
        {
            "location": "/ScikitLearnWrapper/#implementation",
            "text": "__init__(self,\n        learningRate,\n        batchSize,\n        initializer,\n        dropoutRate,\n        restoreFrom,\n        outputLength)  The  __init__  method should be overriden and used to set the hyperparameters for your own model,\nand should call  TFRegressor.__init__  to provide the hyperparameters required by the  TFRegressor .  NOTE: For compatitbility with scikit-learn functionality such as  GridSearchCV , every parameter\npassed to the constructor of your model must be saved to a member variable of exactly the same name.  _buildModelName(self)  In this method you will need to return a string which adequately describes your model and its\nhyperparameters. This string is used to group model files.  _buildGraph(self, numFeatures)  In this method you will need to build the graph for your model, and return a  RegressorTensors \nobject which contains the appropriate tensors/operations from your graph. This object is the\ninterface between the graph which you have created in  _buildGraph  and the training loop\nimplemented in  TFRegressor .   The parameter  numFeatures  is the number of columns in the parameter  X  provided to the  fit \nmethod.  _restoreGraph(self, graph)  This method must also return a  RegressorTensors  object, however the tensors provided to the\nconstructor of  RegressorTensors  must be recovered from the provided  graph  using either graph.get_tensor_by_name(...)  or  graph.get_operation_by_name(...) .  RegressorTensors(self,\n                 X_in,\n                 y_in,\n                 logits,\n                 loss,\n                 trainingOp,\n                 dropoutKeepProb)  The constructor of the  RegressorTensors  object takes several operations from your graph that the\ntraining loop will use while training the model. Each is explained below:   X_in : The  tf.Placeholder  for your features  y_in : The  tf.Placeholder  for your labels  logits : The output of your graph. The predict method will call  logits.eval(...)  to produce predictions  loss : The loss of your graph.  loss.eval(...)  will be called to create training and validation loss values to log in tensorboard  trainingOp : The operator that should be evaluated for each batch and epoch to train your model  dropoutKeepProb : A  tf.Placeholder  which will be set to the dropout rate during training   For complete examples of all of the above methods, see  SKTFModels.BasicRegressor .",
            "title": "Implementation"
        },
        {
            "location": "/ScikitLearnWrapper/#usage",
            "text": "The interface to train models which inherit from  TFRegressor  is the  fit  and  predict  methods.  fit(self, X, y, X_valid, y_valid, numEpochs)  This will call either  _buildGraph  or  _restoreGraph  and then train the model for  numEpochs ,\nusing  X  as the features and  y  as the labels. A validation set must also be provided in  X_valid \nand  y_valid .  predict(self, X)  Will perform inference on the given dataset  X , returning a numpy array of predictions. Handles an X  that has a large number of parameters by splitting it into batches the same size as specified in\nthe constructor.",
            "title": "Usage"
        },
        {
            "location": "/TrainingHelpers/",
            "text": "TrainingHelpers\n\n\nClasses which provide functionality within the training loop of a tensorflow model. To see examples\nof how these classes are used, refer to the \nfit\n method of \nTFRegressor\n.\n\n\nEarlyStoppingHelper\n\n\nImplements early stopping in your model by checking the loss at each epoch.\n\n\n__init__(self, maxChecksWithoutProgress: int)\n\n\n\nConstruct this object shortly before your training loop, where \nmaxChecksWithoutProgress\n is the\nnumber of epochs to continue without a declining loss before \nshouldStop\n returns \nFalse\n.\n\n\nrestoreBestModelParams(self) -> bool\n\n\n\nCall this after your training loop to restore the model to the state which achived the lowest loss.\n\n\nshouldStop(self, lossVal: float) -> bool\n\n\n\nCall this every epoch, providing the validation loss in \nlossVal\n. If \nlossVal\n is lower than any\nprevious value then state of the model will be stored. Returns \nFalse\n if the number of times this\nhas been called without lossVal decreasing has exceeded the value \nmaxChecksWithoutProgress\n as\nprovided in the constructor.\n\n\nProgressCalculator\n\n\nRecords the time taken and estimates the time remaining.\n\n\nTrainingValidator",
            "title": "TrainingHelpers"
        },
        {
            "location": "/TrainingHelpers/#traininghelpers",
            "text": "Classes which provide functionality within the training loop of a tensorflow model. To see examples\nof how these classes are used, refer to the  fit  method of  TFRegressor .",
            "title": "TrainingHelpers"
        },
        {
            "location": "/TrainingHelpers/#earlystoppinghelper",
            "text": "Implements early stopping in your model by checking the loss at each epoch.  __init__(self, maxChecksWithoutProgress: int)  Construct this object shortly before your training loop, where  maxChecksWithoutProgress  is the\nnumber of epochs to continue without a declining loss before  shouldStop  returns  False .  restoreBestModelParams(self) -> bool  Call this after your training loop to restore the model to the state which achived the lowest loss.  shouldStop(self, lossVal: float) -> bool  Call this every epoch, providing the validation loss in  lossVal . If  lossVal  is lower than any\nprevious value then state of the model will be stored. Returns  False  if the number of times this\nhas been called without lossVal decreasing has exceeded the value  maxChecksWithoutProgress  as\nprovided in the constructor.",
            "title": "EarlyStoppingHelper"
        },
        {
            "location": "/TrainingHelpers/#progresscalculator",
            "text": "Records the time taken and estimates the time remaining.",
            "title": "ProgressCalculator"
        },
        {
            "location": "/TrainingHelpers/#trainingvalidator",
            "text": "",
            "title": "TrainingValidator"
        },
        {
            "location": "/FilesAndLogging/",
            "text": "FilesAndLogging\n\n\nSeveral classes which assist with managing model files and creating tensorboard logs.\n\n\nCheckpointAndRestoreHelper\n\n\nFileManager\n\n\nTensorboardLogHelper",
            "title": "FilesAndLogging"
        },
        {
            "location": "/FilesAndLogging/#filesandlogging",
            "text": "Several classes which assist with managing model files and creating tensorboard logs.",
            "title": "FilesAndLogging"
        },
        {
            "location": "/FilesAndLogging/#checkpointandrestorehelper",
            "text": "",
            "title": "CheckpointAndRestoreHelper"
        },
        {
            "location": "/FilesAndLogging/#filemanager",
            "text": "",
            "title": "FileManager"
        },
        {
            "location": "/FilesAndLogging/#tensorboardloghelper",
            "text": "",
            "title": "TensorboardLogHelper"
        },
        {
            "location": "/ModelManager/",
            "text": "ModelManager\n\n\nThe \nModelManager\n is a browser based interface for viewing models and sorting them by\nhyperparameters. It is very early in development, and currently its main use is generating the\ntensorboard command to view models with a chosen set of hyperparameters.\n\n\nYou can start the \nModelManager\n by opening the \nModelManager/index.html\n file, and selecting your\nmodels directory on main page.",
            "title": "ModelManager"
        },
        {
            "location": "/ModelManager/#modelmanager",
            "text": "The  ModelManager  is a browser based interface for viewing models and sorting them by\nhyperparameters. It is very early in development, and currently its main use is generating the\ntensorboard command to view models with a chosen set of hyperparameters.  You can start the  ModelManager  by opening the  ModelManager/index.html  file, and selecting your\nmodels directory on main page.",
            "title": "ModelManager"
        }
    ]
}