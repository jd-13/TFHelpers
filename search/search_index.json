{
    "docs": [
        {
            "location": "/",
            "text": "TFHelpers\n\n\nTFHelpers is a small collection of classes which implement common tasks in Tensorflow. The\nfunctionality provided by each module is described in this documentation.\n\n\nThe modules \nFilesAndLogging\n and \nTrainingHelpers\n provide functionality such as managing model\nfiles and early stopping, and may be useful in many tensorflow models. The module \nScikitWrapper\n\nprovides several classes which aid with implementing tensorflow models behind a scikit-learn style\nAPI.\n\n\nIf you use the \nFilesAndLogging\n module you'll also be able to use the \nModelManager\n web interface\nto sort your models by hyperparameters and generate the appropriate tensorboard commands to compare\ndifferent models and training runs.",
            "title": "Home"
        },
        {
            "location": "/#tfhelpers",
            "text": "TFHelpers is a small collection of classes which implement common tasks in Tensorflow. The\nfunctionality provided by each module is described in this documentation.  The modules  FilesAndLogging  and  TrainingHelpers  provide functionality such as managing model\nfiles and early stopping, and may be useful in many tensorflow models. The module  ScikitWrapper \nprovides several classes which aid with implementing tensorflow models behind a scikit-learn style\nAPI.  If you use the  FilesAndLogging  module you'll also be able to use the  ModelManager  web interface\nto sort your models by hyperparameters and generate the appropriate tensorboard commands to compare\ndifferent models and training runs.",
            "title": "TFHelpers"
        },
        {
            "location": "/SKTFModels/",
            "text": "SKTFModels\n\n\nThis module provides examples of models built using the \nSciKitWrapper\n module. This is a good place\nto start if you wish to implement your own models using this wrapper.\n\n\nBasicRegressor\n\n\nCurrently this module provides only the \nBasicRegressor\n class, which inherits from \nTFRegressor\n\nand implements a basic multilayer feedforward network for regressions. Its hyperparameters are to be\npassed to the constructor as follows, and provide the following options:\n\n\nBasicRegressor(self,\n               learningRate=0.01,\n               batchSize=1000,\n               initializer=tf.contrib.layers.variance_scaling_initializer(),\n               dropoutRate=0.01,\n               restoreFrom=None,\n               hiddenNeuronsList=[10])\n\n\n\n\n\nlearningRate\n: Provided to the gradient descent algorithm, in this case \ntf.train.AdamOptimizer\n\n\nbatchSize\n: Number of rows of data to operate on at once\n\n\ninitializer\n: The initializer to use as the kernel initializer for each layer\n\n\ndropoutRate\n: Not yet implemented in this model, does nothing\n\n\nrestoreFrom\n: If this model has been trained previously with the same hyperparameters, provide\nthe date time string of the form YYYYMMDD-HHmm which corresponds to the previous training run. The\n\nCheckpointAndRestoreHelper\n will then look this up from the models directory.\n\n\nhiddenNeuronsList\n: A list of integers which describes the number of neurons in each hidden layer.\nEg. [100, 50, 30] would mean 100 neurons in the first layer, 50 in the second and 30 in the third\nlayer.",
            "title": "SKTFModels"
        },
        {
            "location": "/SKTFModels/#sktfmodels",
            "text": "This module provides examples of models built using the  SciKitWrapper  module. This is a good place\nto start if you wish to implement your own models using this wrapper.",
            "title": "SKTFModels"
        },
        {
            "location": "/SKTFModels/#basicregressor",
            "text": "Currently this module provides only the  BasicRegressor  class, which inherits from  TFRegressor \nand implements a basic multilayer feedforward network for regressions. Its hyperparameters are to be\npassed to the constructor as follows, and provide the following options:  BasicRegressor(self,\n               learningRate=0.01,\n               batchSize=1000,\n               initializer=tf.contrib.layers.variance_scaling_initializer(),\n               dropoutRate=0.01,\n               restoreFrom=None,\n               hiddenNeuronsList=[10])   learningRate : Provided to the gradient descent algorithm, in this case  tf.train.AdamOptimizer  batchSize : Number of rows of data to operate on at once  initializer : The initializer to use as the kernel initializer for each layer  dropoutRate : Not yet implemented in this model, does nothing  restoreFrom : If this model has been trained previously with the same hyperparameters, provide\nthe date time string of the form YYYYMMDD-HHmm which corresponds to the previous training run. The CheckpointAndRestoreHelper  will then look this up from the models directory.  hiddenNeuronsList : A list of integers which describes the number of neurons in each hidden layer.\nEg. [100, 50, 30] would mean 100 neurons in the first layer, 50 in the second and 30 in the third\nlayer.",
            "title": "BasicRegressor"
        },
        {
            "location": "/ScikitLearnWrapper/",
            "text": "Scikit-learn Wrapper\n\n\nTFHelpers provides a basic wrapper class which you can inherit from to create models which are\ncompatible with the scikit-learn api. Models which inherit from the \nSKTFWrapper\n class will be\ncompatible with scikit-learn functionality such as \nGridSearchCV\n, and will provide \nfit\n and\n\npredict\n methods for training and producing predictions.\n\n\nIf you inherit from \nSKTFWrapper\n you will need to build the graph and implement the training loop\nin the \nfit\n method. The \nSciKitWrapper\n module also contains child classes of \nSKTFWrapper\n which\nprovide further functionality. Currently only \nTFRegressor\n is available.\n\n\nTFRegressor\n\n\nIf you wish to create a regression model you can inherit from the \nTFRegressor\n class which provides\na lot of functionality, leaving the methods \n_buildGraph\n and \n_buildModelName\n for you to\nimplement.\n\n\nUsing \nTFRegressor\n gets you the following features for free:\n\n\n\n\nA fairly conventional training loop, with user specified batch size\n\n\nLogging of training and validation losses to tensorboard on each epoch, as well as writing any \ntf.Summary\n nodes at each epoch\n\n\nSaving the model at each epoch\n\n\nContinuing training from previous runs\n\n\nSaving all model and tensorboard related files in a directory structure sorted by model hyperparameters and start time\n\n\nEarly stopping\n\n\nPredicted time to completion\n\n\nWarnings if issues are detected during training such as: parts of the graph do not appear to be training, the loss is zero, or there is a risk of exploding gradients\n\n\n\n\nImplementation\n\n\n_buildModelName()\n\n\n\nIn this method you will need to return a string which adequately describes your model and its\nhyperparameters. This string is used to group model files.\n\n\n_buildGraph()\n\n\n\nIn this method you will need to build the graph for your model, and return a \nRegressorTensors\n\nobject which contains the appropriate operations from your graph. This object is the interface\nbetween the graph which you have created in \n_buildGraph\n and the training loop implemented in\n\nTFRegressor\n.\n\n\nRegressorTensors(self,\n                 X_in,\n                 y_in,\n                 logits,\n                 loss,\n                 trainingOp,\n                 init,\n                 saver,\n                 dropoutKeepProb)\n\n\n\nThe constructor of the \nRegressorTensors\n object takes several operations from your graph that the\ntraining loop will use while training the model. Each is explained below:\n\n\n\n\nX_in\n: The \ntf.Placeholder\n for your features\n\n\ny_in\n: The \ntf.Placeholder\n for your labels\n\n\nlogits\n: The output of your graph. The predict method will call \nlogits.eval(...)\n to produce predictions\n\n\nloss\n: The loss of your graph. \nloss.eval(...)\n will be called to create training and validation loss values to log in tensorboard\n\n\ntrainingOp\n: The operator that should be evaluated for each batch and epoch to train your model\n\n\ninit\n: The \ntf.global_variables_initializer()\n that should be used to initialise your graph\n\n\nsaver\n: The \ntf.train.Saver()\n that should be used to save the model\n\n\ndropoutKeepProb\n: A \ntf.Placeholder\n which will be set to the dropout rate during training\n\n\n\n\nFor complete examples of both of the above methods, see \nSKTFModels.BasicRegressor\n.",
            "title": "Scikit-learn Wrapper"
        },
        {
            "location": "/ScikitLearnWrapper/#scikit-learn-wrapper",
            "text": "TFHelpers provides a basic wrapper class which you can inherit from to create models which are\ncompatible with the scikit-learn api. Models which inherit from the  SKTFWrapper  class will be\ncompatible with scikit-learn functionality such as  GridSearchCV , and will provide  fit  and predict  methods for training and producing predictions.  If you inherit from  SKTFWrapper  you will need to build the graph and implement the training loop\nin the  fit  method. The  SciKitWrapper  module also contains child classes of  SKTFWrapper  which\nprovide further functionality. Currently only  TFRegressor  is available.",
            "title": "Scikit-learn Wrapper"
        },
        {
            "location": "/ScikitLearnWrapper/#tfregressor",
            "text": "If you wish to create a regression model you can inherit from the  TFRegressor  class which provides\na lot of functionality, leaving the methods  _buildGraph  and  _buildModelName  for you to\nimplement.  Using  TFRegressor  gets you the following features for free:   A fairly conventional training loop, with user specified batch size  Logging of training and validation losses to tensorboard on each epoch, as well as writing any  tf.Summary  nodes at each epoch  Saving the model at each epoch  Continuing training from previous runs  Saving all model and tensorboard related files in a directory structure sorted by model hyperparameters and start time  Early stopping  Predicted time to completion  Warnings if issues are detected during training such as: parts of the graph do not appear to be training, the loss is zero, or there is a risk of exploding gradients",
            "title": "TFRegressor"
        },
        {
            "location": "/ScikitLearnWrapper/#implementation",
            "text": "_buildModelName()  In this method you will need to return a string which adequately describes your model and its\nhyperparameters. This string is used to group model files.  _buildGraph()  In this method you will need to build the graph for your model, and return a  RegressorTensors \nobject which contains the appropriate operations from your graph. This object is the interface\nbetween the graph which you have created in  _buildGraph  and the training loop implemented in TFRegressor .  RegressorTensors(self,\n                 X_in,\n                 y_in,\n                 logits,\n                 loss,\n                 trainingOp,\n                 init,\n                 saver,\n                 dropoutKeepProb)  The constructor of the  RegressorTensors  object takes several operations from your graph that the\ntraining loop will use while training the model. Each is explained below:   X_in : The  tf.Placeholder  for your features  y_in : The  tf.Placeholder  for your labels  logits : The output of your graph. The predict method will call  logits.eval(...)  to produce predictions  loss : The loss of your graph.  loss.eval(...)  will be called to create training and validation loss values to log in tensorboard  trainingOp : The operator that should be evaluated for each batch and epoch to train your model  init : The  tf.global_variables_initializer()  that should be used to initialise your graph  saver : The  tf.train.Saver()  that should be used to save the model  dropoutKeepProb : A  tf.Placeholder  which will be set to the dropout rate during training   For complete examples of both of the above methods, see  SKTFModels.BasicRegressor .",
            "title": "Implementation"
        },
        {
            "location": "/TrainingHelpers/",
            "text": "TrainingHelpers\n\n\nClasses which provide functionality within the training loop of a tensorflow model.",
            "title": "TrainingHelpers"
        },
        {
            "location": "/TrainingHelpers/#traininghelpers",
            "text": "Classes which provide functionality within the training loop of a tensorflow model.",
            "title": "TrainingHelpers"
        },
        {
            "location": "/FilesAndLogging/",
            "text": "FilesAndLogging\n\n\nSeveral classes which assist with managing model files and creating tensorboard logs.",
            "title": "FilesAndLogging"
        },
        {
            "location": "/FilesAndLogging/#filesandlogging",
            "text": "Several classes which assist with managing model files and creating tensorboard logs.",
            "title": "FilesAndLogging"
        },
        {
            "location": "/ModelManager/",
            "text": "ModelManager\n\n\nThe \nModelManager\n is a browser based interface for viewing models and sorting them by\nhyperparameters. It is very early in development, and currently its main use is generating the\ntensorboard command to view models with a chosen set of hyperparameters.\n\n\nYou can start the \nModelManager\n by opening the \nModelManager/index.html\n file, and selecting your\nmodels directory on main page.",
            "title": "ModelManager"
        },
        {
            "location": "/ModelManager/#modelmanager",
            "text": "The  ModelManager  is a browser based interface for viewing models and sorting them by\nhyperparameters. It is very early in development, and currently its main use is generating the\ntensorboard command to view models with a chosen set of hyperparameters.  You can start the  ModelManager  by opening the  ModelManager/index.html  file, and selecting your\nmodels directory on main page.",
            "title": "ModelManager"
        }
    ]
}